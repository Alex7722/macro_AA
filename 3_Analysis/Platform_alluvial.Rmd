---
title: "Overview of All Networks and Clusters"
author: "Aurélien Goutsmedt and Alexandre Truc"
output: 
  html_document:
    theme: united
    toc: true
    number_sections: true
    toc_float: true
    toc_depth: 2
    code_folding: hide
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
 if ("knitr" %in% installed.packages() == FALSE) {
    install.packages(knitr, dependencies = TRUE)
  }
library(knitr)
 if ("kableExtra" %in% installed.packages() == FALSE) {
    install.packages(kableExtra, dependencies = TRUE)
  }
library(kableExtra)
library(here)
```

```{css, echo=FALSE}
body .main-container {
max-width: 100% !important;
width: 100% !important;
margin: auto;
}
body {
max-width: 100% !important;
}
```

```{css zoom-lib-src, echo = FALSE}
# Follows the css and js script used for allow zooming in graphs
script src = "https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"
```

```{js zoom-jquery, echo = FALSE}
 $(document).ready(function() {
    $('body').prepend('<div class=\"zoomDiv\"><img src=\"\" class=\"zoomImg\"></div>');
    // onClick function for all plots (img's)
    $('img:not(.zoomImg)').click(function() {
      $('.zoomImg').attr('src', $(this).attr('src')).css({width: '100%'});
      $('.zoomDiv').css({opacity: '1', width: 'auto', border: '1px solid white', borderRadius: '5px', position: 'fixed', top: '50%', left: '50%', marginRight: '-50%', transform: 'translate(-50%, -50%)', boxShadow: '0px 0px 50px #888888', zIndex: '50', overflow: 'auto', maxHeight: '100%'});
    });
    // onClick function for zoomImg
    $('img.zoomImg').click(function() {
      $('.zoomDiv').css({opacity: '0', width: '0%'}); 
    });
  });
```

```{r loading files, set.seed(3155210), echo=FALSE}
#' ---
#' title: "Alluvial html for the plaform"
#' author: "Aurélien Goutsmedt and Alexandre Truc"
#' date: "/ Last compiled on `r format(Sys.Date())`"
#' output: 
#'   github_document:
#'     toc: true
#'     number_sections: true
#' ---
#' 


#+ r setup, include = FALSE
knitr::opts_chunk$set(eval = FALSE)

#' # Loading packages, paths and data

#' We first load all the functions created in the functions script. 
#' We also have a separated script with all the packages needed for the scripts
#' in the `dynamic_networks` directory, as well as all the paths (for saving
#' pictures for instance). This separated script also loads our data 
#' and color palettes.

#+ r source
source(here::here("functions","functions_dynamics_networks_alex.R"))
# source("functions/functions_networks_alex.R")
source(here::here("functions","functions_for_network_analysis.R"))
source(here::here("functions","Script_paths_and_basic_objectsV2.R"))

authors_JEL <- readRDS(here(data_path,"macro_AA","3_Corpus_WoS","JEL_matched_corpus_authors.rds"))
ref_info_JEL <- readRDS(here(data_path,"macro_AA","3_Corpus_WoS","JEL_matched_corpus_references_info.rds"))
institutions <- readRDS(here(data_path,"macro_AA","3_Corpus_WoS","MACRO_AA_INSTITUTIONS.rds"))


# keepint only refs with a title and a ESpecialite, then removing doublons
ref_info_JEL <- unique(ref_info_JEL[Titre != "NA" & ESpecialite != "NA", c("ItemID_Ref", "Titre", "ESpecialite")])
doublons <- which(duplicated(ref_info_JEL$New_id2))

if (length(doublons) != 0) {
  ref_info_JEL <- ref_info_JEL[-doublons]
}

# Adding info to references
edges_JEL <- edges_JEL %>% mutate(ID_Art = as.character(ID_Art),
                                   New_id2 = as.character(New_id2),
                                  ItemID_Ref = as.character(ItemID_Ref))
ref_info_JEL <- ref_info_JEL %>% mutate(ItemID_Ref = as.character(ItemID_Ref))
nodes_JEL <- nodes_JEL %>% mutate(ID_Art = as.character(ID_Art),
                                  ItemID_Ref = as.character(ItemID_Ref))
authors_JEL <- authors_JEL %>% mutate(ID_Art = as.character(ID_Art))
edges_JEL <- merge(unique(edges_JEL[New_id2!=0]), unique(ref_info_JEL[Titre != "NA" & ESpecialite != "NA", c("ItemID_Ref", "Titre", "ESpecialite")]), by = "ItemID_Ref", all.x = TRUE)
edges_JEL <- merge(edges_JEL, unique(nodes_JEL[, c("ID_Art", "Annee_Bibliographique")]), by = "ID_Art", all.x = TRUE)

# removing useless files

rm(list = c("ref_info_JEL","ref_info_old_JEL","authors_old_JEL"))


alluv_dt <- readRDS(here(graph_data_path, "alluv_dt_1969-2011.rds"))

alluv_dt <- readRDS(here(graph_data_path, "alluv_dt_named_colored.rds"))
alluv_dt <- alluv_dt[, Com_ID := new_Id_com]
alluv_dt <- alluv_dt[, share_max := max(share_leiden), by = "Com_ID"]
alluv_dt <- alluv_dt[, color :=color_nodes]
alluv_dt[,Window_plot:=paste0(Window, "-\n", as.numeric(Window)+time_window-1)]

plot_alluvial <- ggplot(alluv_dt, aes(x = Window_plot, y=share, stratum = new_Id_com, alluvium = ID_Art, fill = color_nodes)) +
  geom_stratum(alpha =1, size=1/10) +
  geom_flow() +
  theme(legend.position = "none") +
  theme_minimal() +
  theme(plot.background = element_rect(fill = 'white', colour = NA)) +
  scale_fill_identity() +
  ggtitle("") +
  ggrepel::geom_label_repel(stat = "stratum", size = 5, aes(label = Label), max.overlaps = 5)
ggsave(here(picture_path, "alluvial_platform_graph.png"), plot = plot_alluvial, width = 60, height = 50, units = "cm")

plot_alluvial_noname <- ggplot(alluv_dt, aes(x = Window_plot, y=share, stratum = new_Id_com, alluvium = ID_Art, fill = color_nodes)) +
  geom_stratum(alpha =1, size=1/10) +
  geom_flow() +
  theme(legend.position = "none") +
  theme_minimal() +
  theme(plot.background = element_rect(fill = 'white', colour = NA)) +
  scale_fill_identity() +
  ggtitle("") 
  # ggrepel::geom_label_repel(stat = "stratum", size = 5, aes(label = Label), max.overlaps = 5)
ggsave(here(picture_path, "alluvial_platform_graph_noname.png"), plot = plot_alluvial_noname, width = 60, height = 50, units = "cm")

tf_idf_alluvial <- tf_idf(nodes = alluv_dt[color != "#B2B2B2"],
                         com_name_column = "new_Id_com",
                         number_of_words = 20, 
                         threshold_com = 0.03,
                         com_size_column = "share_max",
                         size_title_wrap = 10)

alluv_dt <- alluv_dt[color != "#B2B2B2"]
alluv_dt <- merge(alluv_dt, nodes_JEL[,c("ID_Art","ItemID_Ref")], by = "ID_Art")




```

In this section, you will find a graphic representation of all networks and communities in an alluvial diagram, as well as detailed information on the different inter-temporal clusters. You can click on images to zoom and explore the figures with more precision.

> Please read first the [technical documentation](../documentation/)

# Corpus Distribution

```{r histogram_plot, eval=TRUE, echo=FALSE}


plot_hist <- ggplot(nodes_JEL[,.N,Annee_Bibliographique], aes(x=Annee_Bibliographique, y=N)) +
  geom_bar(stat="identity")+
  # geom_text(aes(label=N), vjust=-0.3, size=3.5)+
  theme_minimal(base_size = 22) +
  theme(plot.background = element_rect(fill = 'white', colour = NA)) +
  labs(y = "Number of Observations", x = "Year") +
  ggtitle("") 
ggsave(here(picture_path, "corpus_histogram.png"), plot = plot_hist, width = 60, height = 50, units = "cm")


include_graphics(here(picture_path,"corpus_histogram.png"))


```

# Alluvial Diagram

The alluvial diagram offer a synthetic representation of our networks and the different communities identified. 

-   The X axis delimits the different time windows of the platform. For one time window, you find all the communities in the network represented as rectangles (the stratum of the alluvial);

-   Each vertical rectangle represents a cluster in a given network detected by the Leiden algorithm. Rectangles are given a color according to inter-temporal identification of this cluster and its qualitative naming (see [technical documentation](../documentation/#Cluster_detection) for further details). Colors in the alluvial directly match colors in the graphs representation on the platform.

-   The Y axis is both *(i)* the share of nodes belonging to a given community for rectangles and *(ii)* the share of nodes flowing between clusters in successive time windows (see next point).

-   The "flow" between rectangles (the alluvium) is the share of nodes going from one cluster to the other. On the upper part of each rectangle, the absence of flow on the right side is the share of nodes that disappear in the next time window network (articles published in the first year of the current time window and that disappear in the next time window). The absence of flow on the left side of the rectangle is the share of nodes not found in the previous time window network (articles published in the last year of the current time window and that were not yet published in the previous time window).


```{r alluvial_plot, eval=TRUE, echo=FALSE}

include_graphics(here(picture_path,"alluvial_platform_graph.png"))

```

# List of the main macroeconomics communities in the alluvial

In this section, we provide a number of synthetic information and tables about [**intertemporal communities**](../documentation/#Cluster_detection). On the [Graph](../) page, by clicking on *Community information* button, you have access on information about the community for the displayed time-window. In what follows, you have access to information about the community as a whole, that is for all the time windows in which this community exists. For each intertemporal community, you can find:

-   The first year of the five-year window in which the community has been detected for the first time and the last year of the last window before it disappears;

-   The number of unique articles (and its share on the whole corpus), as well as the number of nodes in the community (and its share on the sum of nodes). Indeed, different measures are necessary as the networks are built on five-year windows: each article appears in five networks. Consequently, the number of articles and of nodes in a cluster is not the same. Besides, an article may appear in different communities for different time windows.   We also measure the share of nodes in a community for the time window when the community is the largest in proportion;

-   The five authors that have published the highest number of articles in the community;

-   The five journals that have published the highest number of articles in the community;

-   The articles of the community that "identifies" the most the community. We use a normalized deviation measure, which takes into account the number of times an article is cited in the corpus in general during the period---if the community existed from 1979 to 1991, we calculate the number of citations of this article, by macroeconomics article, between 1979 and 1991---and the number of times it is cited by other articles of the community. The measure looks like this: $\theta = \sqrt{N}\frac{f - f_0}{\sqrt{f_0(1 - f_0)}}$ with $N$ the total number of articles in the corpus in the period, $f$ the weighted frequency in the community and $f0$ the weighted frequency in the whole corpus. The articles with the highest values are the articles which are highly cited by other articles of the community, without being highly cited by articles of the period in other communities.

-   The most cited references (articles, books, book chapters...).

-   The words and bigrams (in articles' titles) that "identify" the most the communities. We use the [Term-Frequency/Inverse-Document-Frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) measure. The Term-Frequency part simply looks at the number of time a word is used (above the total number of words used by the community) in the titles of a community, whereas the Inverse-Document-Frequency calculates in how many communities a word appears. The higher the number of communities it appears in, the lower the IDF will be. In other words, a term has a high TF-IDF value if it is used a lot in a community, but not used in many other communities.


[**IMPORTANT DISCLAIMER**]{.underline}: because the number of articles (nodes) increase rapidly over time, communities that exists over many time windows are mostly characterized in the following tables by the more recent time windows. In turn, the following tables can hide heterogeneity and great transformations within a particular community simply because the weight of the more recent time windows is simply too important. [The best exploratory tool remain to look at each community on the platform for different time window]{.underline}.

```{r,results = "asis", eval=TRUE, echo=FALSE}
for (com in unique(alluv_dt[order(ID_bis)]$Com_ID)) {
  ####################### Preparing the data to put in the template
  
  # restricting alluv_dt to the community at stake
  alluv_com  <- alluv_dt[Com_ID == com]
  # extracting the first year and last year of the community
  window <- as.integer(c(min(unique(alluv_com$Window)), as.integer(max(unique(alluv_com$Window))) + (time_window - 1)))

  # number of citations of citing articles and references
  nb_cit <- edges_JEL[ID_Art %in% nodes_JEL[between(Annee_Bibliographique, window[1],window[2])]$ID_Art][,nb_cit := .N, by = "ItemID_Ref"][, total_cit := .N] # this is the most cited nodes in the period / We also compute the total number of citations on the period
  refs_alluv <- edges_JEL[ID_Art %in% alluv_com$ID_Art][,nb_cit_com := .N, by = "New_id2"][, c("New_id2","ItemID_Ref","Nom","Annee","Revue_Abbrege","Titre","nb_cit_com")][, total_cit_com := .N] # this is the most cited ref by the nodes of the community / We also compute the total number of citations by the community
  alluv_com <- merge(alluv_com, unique(nb_cit[,c("ItemID_Ref","nb_cit","total_cit")]), by = "ItemID_Ref", all.x = TRUE)
  alluv_com <- merge(alluv_com, nodes_JEL[,c("ID_Art","Annee_Bibliographique","Nom","Revue")], by = "ID_Art", all.x = TRUE)
  authors_alluv <- merge(alluv_com[,c("ID_Art","Com_ID")], authors_JEL[,c("ID_Art","Nom","Ordre")], by = "ID_Art")
  
# saving the number of articles
nb_art <- length(unique(nb_cit$ID_Art))
# cleaning useless files  
rm("nb_cit")

# Keeping the n nodes with the highest number of citations in our corpus, per community
most_cited_nodes <- unique(alluv_com[, c("ItemID_Ref","Nom","Annee_Bibliographique","Titre","nb_cit","total_cit")])
most_cited_nodes <- merge(most_cited_nodes, unique(refs_alluv[,c("ItemID_Ref","nb_cit_com","total_cit_com")]), by = "ItemID_Ref", all.x = TRUE)

# a bit of cleaning: summing the number of citation of doublons / transforming NA value to 0
most_cited_nodes <- unique(most_cited_nodes[, nb_cit_com := sum(nb_cit_com), by = "ItemID_Ref"])
most_cited_nodes[is.na(nb_cit_com)]$nb_cit_com <- 0

# tf-idf measure of the highest cited nodes
most_cited_nodes <- most_cited_nodes[, `:=` (com_freq = nb_cit_com/total_cit_com, total_freq = nb_cit/total_cit)][,norm_dev_ref := sqrt(nb_art)*(com_freq - total_freq)/sqrt(total_freq*(1-total_freq))][,c("Nom","Annee_Bibliographique","Titre","norm_dev_ref")]


most_cited_nodes <- most_cited_nodes %>%
  arrange(desc(norm_dev_ref)) %>%
  slice(1:10)

# Most cited reference in the community
doublons <- which(duplicated(refs_alluv[,c("New_id2")]))

if(length(doublons) != 0){
  refs_alluv <- refs_alluv[-doublons]
}

most_cited_ref <-  refs_alluv %>%
  select(-New_id2) %>% 
  unique() %>% 
  arrange(desc(nb_cit_com)) %>%
  slice(1:15) %>% as.data.table()
most_cited_ref[, share_of_paper_citing:=paste0(format(round(nb_cit_com/alluv_com[,.N,ID_Art][,.N]*100,2), 2 ),"%")]

# keeping the n authors the most present in coupling communities
authors_alluv <- authors_alluv[, nb_art := .N, by = "Nom"]
main_author <- authors_alluv %>%
  select(Nom,nb_art) %>% 
  unique() %>%
  arrange(desc(nb_art)) %>%
  slice(1:5)

# keeping the n more important journals
main_journals <- unique(alluv_com[, c("ID_Art","Revue")])
main_journals <- main_journals[, nb_art := .N, by = "Revue"]
main_journals <- main_journals %>%
  select(Revue,nb_art) %>% 
  unique() %>% 
  arrange(desc(nb_art)) %>%
  slice(1:5)

# occurrence of countries
main_countries <- institutions[ID_Art %in% alluv_com$ID_Art][,.SD[1], .(Pays, ID_Art)][,.N,Pays][,share:=N/sum(N)][order(-share)][,share_format:=paste0(format(round(share*100,2), 2),"%")]

# occurrence of institutions
main_institutions <- institutions[ID_Art %in% alluv_com$ID_Art][,.SD[1], .(Institution, ID_Art)][,.N,Institution][,share:=N/sum(N)][order(-share)][,share_format:=paste0(format(round(share*100,2), 2),"%")]

################ Beginning of the template ######################
  cat(sprintf("  \n## Community _%s_ \n\n", unique(alluv_com[Com_ID == com]$`sub-name`)))
  cat(paste0("  \nThe community exists from ", window[1]," to ", window[2],". \n"))
  cat(paste0("  \nThe community gathers ",length(unique(alluv_com$ID_Art))," unique articles from our corpus (",round(length(unique(alluv_com$ID_Art))/length(unique(alluv_dt$ID_Art))*100,2),"% of the corpus). The nodes of the community represents ", round(unique(alluv_com$share_leiden_total)*100,2), "% of all nodes across all networks. In the five-year window when its share in the network is the biggest, it represents ", round(unique(alluv_com$share_max)*100,2), "% of the corresponding five-year network. \n"))
  cat(paste0("  \nThe five most prolific authors in this community are ",paste0(main_author$Nom," (",main_author$nb_art,")", collapse = ", "),". \n"))
  cat(paste0("  \nThe five journals with the highest number of articles in this community are ",paste0(main_journals$Revue," (",main_journals$nb_art,")", collapse = ", "),". \n"))

  
  cat(sprintf("  \n### Most cited articles of the community _%s_ \n\n", unique(alluv_com[Com_ID == com]$`sub-name`)))
  cat(paste0("In the next table, you have the ten articles of the community with the highest normalised deviation measure, which compares the number of citations of these ten articles by the community itself, with the number of citations by the whole corpus between ", window[1]," and ", window[2],". \n\n"))
  
  print(kbl(most_cited_nodes,
            col.names = c("First Author","Year","Title","Citations Deviation")) %>% 
  kable_styling(bootstrap_options = c("striped","condensed")))
  
  cat(sprintf("  \n### Most cited references by the community _%s_ \n\n", unique(alluv_com[Com_ID == com]$`sub-name`)))
  cat(paste0("In the next table, you have the fifteen most cited references by the articles of the community. \n\n"))
  
  print(kbl(most_cited_ref[,c("Nom","Annee","Revue_Abbrege","Titre","nb_cit_com","share_of_paper_citing")],
            col.names = c("First Author","Year","Book title or journal (abbrv.)","Title","Number of Citations","Share of Papers Citing")) %>% 
        kable_styling(bootstrap_options = c("striped","condensed")))
  
  cat(sprintf("  \n### Terms with the highest TF-IDF value in community %s \n\n", unique(alluv_com[Com_ID == com]$`sub-name`)))
  cat(paste0("  \nWe have extracted all the terms (unigrams and bigrams, that is one word and two words) in the title of all the community articles. We display here the 20 terms which indentify the most this particular community, in comparison to the other communities.\n\n"))
plot(ggplot(tf_idf_alluvial$list_words[Com_ID == com], aes(reorder_within(word, tf_idf, color), tf_idf, fill = color)) +
    geom_bar(stat = "identity", alpha = .8, show.legend = FALSE) +
    labs(
      title = "Highest tf-idf",
      x = "Words", y = "tf-idf"
    ) +
    scale_fill_identity() +
    scale_x_reordered() +
    coord_flip() +
    theme_minimal())

  cat(sprintf("  \n### Country Occurences in Affiliations %s \n\n", unique(alluv_com[Com_ID == com]$`sub-name`)))
  cat(paste0("  \nWe computed the share of authors' affiliations to particular countries. While the information is extracted from Authors' affiliations, note that the occurence is computed on unique countries affiliations per paper (e.g., an affiliation to Germany is counted as one occurence in a paper no matter the number of authors affiliated to German institutions, or the number of individual affiliations to different German institutions.\n\n"))
  
  print(kbl(main_countries[1:10][,c("Pays", "N", "share_format")],
            col.names = c("Country", "Number of Occurences", "Share of Occurence")) %>% 
        kable_styling(bootstrap_options = c("striped","condensed")))
  
  cat(sprintf("  \n### Institutions Occurences in Affiliations %s \n\n", unique(alluv_com[Com_ID == com]$`sub-name`)))
  cat(paste0("  \nWe computed the share of authors' affiliations to particular institutions. While the information is extracted from Authors' affiliations, note that the occurence is computed on unique institutions affiliations per paper (e.g., an affiliation to Harvard is counted as one occurence in a paper no matter the number of authors affiliated to Harvard, or the number of individual affiliations to different Harvard departments"))
  
  print(kbl(main_institutions[1:10][,c("Institution", "N", "share_format")],
            col.names = c("Institution", "Number of Occurences", "Share of Occurence")) %>% 
        kable_styling(bootstrap_options = c("striped","condensed")))

  cat("  \n")
}
```
